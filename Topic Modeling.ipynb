{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cole Bailey\n",
    "\n",
    "### cole.bailey@sandiego.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 5.1: Topic Modeling\n",
    "\n",
    "This notebook holds Assignment 5.1 for Module 5 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In this assignment you will work with a categorical corpus that accompanies `nltk`. You will build the three types of topic models described in Chapter 8 of _Blueprints for Text Analytics using Python_: NMF, LSA, and LDA. You will compare these models to the true categories. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:29: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from tensorflow.python.lib.core import _pywrap_bfloat16\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:511: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object,\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:553: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object,\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:563: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool,\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object:\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:110: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool:\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/numpy_ops/np_random.py:110: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def randint(low, high=None, size=None, dtype=onp.int):  # pylint: disable=missing-function-docstring\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.object, string),\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:570: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.bool, bool),\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:594: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  types_pb2.DT_STRING: np.object,\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:598: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  types_pb2.DT_BOOL: np.bool,\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:615: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  types_pb2.DT_STRING_REF: np.object,\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:620: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  types_pb2.DT_BOOL_REF: np.bool,\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorboard/util/tensor_util.py:109: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object: SlowAppendObjectArrayToTensorProto,\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/tensorboard/util/tensor_util.py:110: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool: SlowAppendBoolArrayToTensorProto,\n"
     ]
    }
   ],
   "source": [
    "# These libraries may be useful to you\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#import gensim\n",
    "#import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "#from gensim.models import CoherenceModel,LdaMulticore, Phrases \n",
    "#from gensim.models.phrases import Phraser \n",
    "#from gensim.corpora import Dictionary\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "from spacy.lang.en.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional\n",
    "import spacy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function comes from the BTAP repo.\n",
    "\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to Know the Brown Corpus\n",
    "\n",
    "Let's spend a bit of time getting to know what's in the Brown corpus, our NLTK example of an \"overlapping\" corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/colebailey/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For adventure we have 29 articles.\n",
      "For belles_lettres we have 75 articles.\n",
      "For editorial we have 27 articles.\n",
      "For fiction we have 29 articles.\n",
      "For government we have 30 articles.\n",
      "For hobbies we have 36 articles.\n",
      "For humor we have 9 articles.\n",
      "For learned we have 80 articles.\n",
      "For lore we have 48 articles.\n",
      "For mystery we have 24 articles.\n",
      "For news we have 44 articles.\n",
      "For religion we have 17 articles.\n",
      "For reviews we have 17 articles.\n",
      "For romance we have 29 articles.\n",
      "For science_fiction we have 6 articles.\n"
     ]
    }
   ],
   "source": [
    "# categories of articles in Brown corpus\n",
    "for category in brown.categories() :\n",
    "    print(f\"For {category} we have {len(brown.fileids(categories=category))} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataframe of the articles in of hobbies, editorial, government, news, and romance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = ['editorial','government','news','romance','hobbies'] \n",
    "\n",
    "category_list = []\n",
    "file_ids = []\n",
    "texts = []\n",
    "\n",
    "for category in categories : \n",
    "    for file_id in brown.fileids(categories=category) :\n",
    "        \n",
    "        # build some lists for a dataframe\n",
    "        category_list.append(category)\n",
    "        file_ids.append(file_id)\n",
    "        \n",
    "        text = brown.words(fileids=file_id)\n",
    "        texts.append(\" \".join(text))\n",
    "\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame()\n",
    "df['category'] = category_list\n",
    "df['id'] = file_ids\n",
    "df['text'] = texts \n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some helpful columns on the df\n",
    "df['char_len'] = df['text'].apply(len)\n",
    "df['word_len'] = df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f948709cb50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df.groupby('category').agg({'word_len': 'mean'}).plot.bar(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do our TF-IDF and Count vectorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(166, 4941)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_text_vectorizer = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(df[\"text\"])\n",
    "count_text_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 4941)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['text'])\n",
    "tfidf_text_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What do the two data frames `count_text_vectors` and `tfidf_text_vectors` hold? \n",
    "\n",
    "A: TheTFIDF text vectors stems from a bag of word model. It holds the words most relevant to the categories in the corpus. The inverse document frequency can be used to measure the important or unimportance of words in the text. Count text vectors works by one hot encoding the words in the topic. It will then produce a sum of how often the specific word appears in specific topics. Both methods are useful for determining text relevancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Non-Negative Matrix Factorization Model\n",
    "\n",
    "In this section the code to fit a five-topic NMF model has already been written. This code comes directly from the [BTAP repo](https://github.com/blueprints-for-text-analytics-python/blueprints-text), which will help you tremendously in the coming sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/scipy/linalg/decomp_qr.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  kwargs['lwork'] = ret[-2][0].real.astype(numpy.int)\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/scipy/linalg/decomp_qr.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  kwargs['lwork'] = ret[-2][0].real.astype(numpy.int)\n"
     ]
    }
   ],
   "source": [
    "nmf_text_model = NMF(n_components=5, random_state=314)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  mr (0.51)\n",
      "  president (0.45)\n",
      "  kennedy (0.43)\n",
      "  united (0.42)\n",
      "  khrushchev (0.40)\n",
      "\n",
      "Topic 01\n",
      "  said (0.88)\n",
      "  didn (0.46)\n",
      "  ll (0.45)\n",
      "  thought (0.42)\n",
      "  man (0.37)\n",
      "\n",
      "Topic 02\n",
      "  state (0.40)\n",
      "  development (0.36)\n",
      "  tax (0.33)\n",
      "  sales (0.30)\n",
      "  program (0.25)\n",
      "\n",
      "Topic 03\n",
      "  mrs (2.61)\n",
      "  mr (0.78)\n",
      "  said (0.64)\n",
      "  miss (0.52)\n",
      "  car (0.51)\n",
      "\n",
      "Topic 04\n",
      "  game (1.01)\n",
      "  league (0.74)\n",
      "  ball (0.72)\n",
      "  baseball (0.71)\n",
      "  team (0.66)\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some work for you to do. Compare the NMF factorization to the original categories from the Brown Corpus.\n",
    "\n",
    "The NMF topics are mostly similar to the Brown Corpus topics. There are similarities between the topics and the categories created in the DF. By evaluating the top words in the topics, the user can see how one could create categories such as hobbies, government, and news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#The original categories from the BROWN CORPUS were hobbies,editorial, government, news & romance\n",
    "#the numbers are the percentage contributions of the words to the respective topic\n",
    "#If the percentage within a topic is rapidly decreasing, the topic is well-defined, \n",
    "##whereas slowly decreasing word probabilities indicate a less-pronounced topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How does your five-topic NMF model compare to the original Brown categories? \n",
    "\n",
    "A: The five topic NMF model focuses on the most frequent/relevant words rather than the individual categories present in the created dataframe. The topics do not seem to be directly tied to the created categories. Topic 00 uses words one could expect to hear in a government category. 'President' and ' Kennedy' directly tie to the U.S government while topic 04 has the most frequent words of 'baseball' and 'game' which could easily fit into the category of hobbies. These topics do not seem to be clearly well defined because of the low frequency of these words as well as the failure of these words to decrease in length rather quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an LSA Model\n",
    "\n",
    "In this section, follow the example from the repository and fit an LSA model (called a \"TruncatedSVD\" in `sklearn`). Again fit a five-topic model and compare it to the actual categories in the Brown corpus. Use the TF-IDF vectors for your fit, as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/scipy/linalg/decomp_qr.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  kwargs['lwork'] = ret[-2][0].real.astype(numpy.int)\n",
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/scipy/linalg/decomp_qr.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  kwargs['lwork'] = ret[-2][0].real.astype(numpy.int)\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "lsa_text_model = sklearn.decomposition.TruncatedSVD(n_components=5, random_state=314)\n",
    "W_text_matrix = lsa_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = lsa_text_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How does your five-topic LSA model compare to the original Brown categories? \n",
    "\n",
    "A: The LSA model has much more frequency in some of the words. In topic 03, 'mrs' has a 29.45 appearance rate. Many of these words could fit into the original brown corpus categories. Again, there are words that could appear in the government and hobby categories (Kennedy, Khrushchev, baseball...). However, many of these words are incredibly generic and are not in any well defined topic. The low frequency is a strong indicator that these topics are not well defined. There are also no words that appear that the user could confidently place in a romance category. 'mrs' appears in many topics but its hard to understand its high frequency in a topic that also includes 'game' and 'university'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  said (0.44)\n",
      "  mr (0.25)\n",
      "  mrs (0.22)\n",
      "  state (0.20)\n",
      "  man (0.17)\n",
      "\n",
      "Topic 01\n",
      "  said (3.89)\n",
      "  ll (2.73)\n",
      "  didn (2.63)\n",
      "  thought (2.20)\n",
      "  got (1.97)\n",
      "\n",
      "Topic 02\n",
      "  mrs (3.12)\n",
      "  mr (1.70)\n",
      "  said (1.06)\n",
      "  kennedy (0.82)\n",
      "  khrushchev (0.77)\n",
      "\n",
      "Topic 03\n",
      "  mrs (29.45)\n",
      "  club (6.53)\n",
      "  game (6.12)\n",
      "  jr (5.60)\n",
      "  university (5.20)\n",
      "\n",
      "Topic 04\n",
      "  game (4.54)\n",
      "  league (3.27)\n",
      "  baseball (3.22)\n",
      "  ball (3.10)\n",
      "  team (2.94)\n"
     ]
    }
   ],
   "source": [
    "# call display_topics on your model\n",
    "\n",
    "display_topics(lsa_text_model, tfidf_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is your interpretation of the display topics output? \n",
    "\n",
    "A: As shown in the comparison with the brown corpus, there do appear to be issues with the topics being well defined. The extremely high relevance of 'mrs' in topic 03 is confusing since it appears with words such as university game and jr. I'm not sure what name could be given to that topic. However, the words in these topics appear much more frequently with respects to their topics than the words in the NMF model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an LDA Model\n",
    "\n",
    "Finally, fit a five-topic LDA model using the count vectors (`count_text_vectors` from above). Display the results using `pyLDAvis.display` and describe what you learn from that visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your LDA model here\n",
    "lda_text_model = sklearn.decomposition.LatentDirichletAllocation(n_components=5, random_state=314)\n",
    "W_text_matrix = lda_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = lda_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  plastic (0.02)\n",
      "  toes (0.02)\n",
      "  fed (0.02)\n",
      "  panels (0.02)\n",
      "  lumber (0.02)\n",
      "\n",
      "Topic 01\n",
      "  plastic (0.02)\n",
      "  toes (0.02)\n",
      "  fed (0.02)\n",
      "  panels (0.02)\n",
      "  lumber (0.02)\n",
      "\n",
      "Topic 02\n",
      "  said (0.23)\n",
      "  mr (0.17)\n",
      "  state (0.16)\n",
      "  mrs (0.15)\n",
      "  president (0.12)\n",
      "\n",
      "Topic 03\n",
      "  plastic (0.02)\n",
      "  toes (0.02)\n",
      "  fed (0.02)\n",
      "  panels (0.02)\n",
      "  lumber (0.02)\n",
      "\n",
      "Topic 04\n",
      "  didn (0.17)\n",
      "  said (0.15)\n",
      "  ll (0.15)\n",
      "  thought (0.15)\n",
      "  looked (0.14)\n"
     ]
    }
   ],
   "source": [
    "# Call `display_topics` on your fitted model here\n",
    "display_topics(lda_text_model, tfidf_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What inference do you draw from the displayed topics for your LDA model? \n",
    "\n",
    "A: The LDA model produces the lowest relevance numbers of the three models. Three of the five topics have no words with more than 0.02 percentage contribution to the topic. This is extremely indicative that these topics are poorly defined. There would need to be a sharp dropoff in these numbers to show a well-pronounced topic.\n",
    "\n",
    "\n",
    "Q: How does your five-topic LDA model compare to the original Brown categories? \n",
    "\n",
    "A: The five topics from the LDA model are similar to the Brown categories but not exact. It is difficult to place any of these topics as romance or hobbies. The LDA model also created three topics with the same words. Topic 02 would match the government category as it has the word 'president' but that is about as close as it gets. As far as effectiveness, the LDA was the least effective out of all techniques in matching the Brown Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colebailey/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "lda_display = pyLDAvis.sklearn.prepare(lda_text_model, count_text_vectors, count_text_vectorizer, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el226281402759084268806906236936\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el226281402759084268806906236936_data = {\"mdsDat\": {\"x\": [-0.003611027000647536, -0.003611027000710128, 0.03029039743799715, -0.0036110270007768852, -0.019457316435862584], \"y\": [-0.0012281948361448404, -0.001228194838280074, 0.00117366182492206, -0.001228194839182286, 0.0025109226886851246], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [0.02664141475790898, 0.026641414757911853, 96.63547273573914, 0.026641414757913356, 3.284603019987125]}, \"tinfo\": {\"Term\": [\"said\", \"mr\", \"state\", \"mrs\", \"president\", \"united\", \"000\", \"states\", \"american\", \"man\", \"government\", \"people\", \"city\", \"world\", \"house\", \"program\", \"home\", \"good\", \"development\", \"tax\", \"board\", \"day\", \"old\", \"work\", \"school\", \"week\", \"john\", \"kennedy\", \"car\", \"national\", \"limp\", \"rang\", \"sloping\", \"hated\", \"hey\", \"marry\", \"worried\", \"gently\", \"eyed\", \"shook\", \"smelled\", \"sprinkling\", \"asleep\", \"laughing\", \"smiling\", \"bottle\", \"lips\", \"closet\", \"chill\", \"dirty\", \"fork\", \"awake\", \"skin\", \"sleep\", \"guessed\", \"smart\", \"cleaned\", \"bathroom\", \"touching\", \"flush\", \"plastic\", \"toes\", \"fed\", \"panels\", \"lumber\", \"ham\", \"till\", \"salt\", \"cattle\", \"feed\", \"animal\", \"brush\", \"knees\", \"pipes\", \"pale\", \"drinking\", \"holes\", \"installation\", \"thickness\", \"inexpensive\", \"milk\", \"muscles\", \"width\", \"lengths\", \"grip\", \"windows\", \"tapered\", \"chart\", \"alternate\", \"limp\", \"rang\", \"sloping\", \"hated\", \"hey\", \"marry\", \"worried\", \"gently\", \"eyed\", \"shook\", \"smelled\", \"sprinkling\", \"asleep\", \"laughing\", \"smiling\", \"bottle\", \"lips\", \"closet\", \"chill\", \"dirty\", \"fork\", \"awake\", \"skin\", \"sleep\", \"guessed\", \"smart\", \"cleaned\", \"bathroom\", \"touching\", \"flush\", \"plastic\", \"toes\", \"fed\", \"panels\", \"lumber\", \"ham\", \"till\", \"salt\", \"cattle\", \"feed\", \"animal\", \"brush\", \"knees\", \"pipes\", \"pale\", \"drinking\", \"holes\", \"installation\", \"thickness\", \"inexpensive\", \"milk\", \"muscles\", \"width\", \"lengths\", \"grip\", \"windows\", \"tapered\", \"chart\", \"alternate\", \"mr\", \"state\", \"mrs\", \"president\", \"united\", \"000\", \"states\", \"american\", \"government\", \"city\", \"world\", \"program\", \"development\", \"tax\", \"people\", \"board\", \"school\", \"week\", \"john\", \"kennedy\", \"national\", \"home\", \"car\", \"public\", \"work\", \"service\", \"business\", \"high\", \"men\", \"company\", \"house\", \"said\", \"day\", \"good\", \"man\", \"old\", \"way\", \"limp\", \"rang\", \"sloping\", \"hated\", \"hey\", \"marry\", \"worried\", \"gently\", \"eyed\", \"shook\", \"smelled\", \"sprinkling\", \"asleep\", \"laughing\", \"smiling\", \"bottle\", \"lips\", \"closet\", \"chill\", \"dirty\", \"fork\", \"awake\", \"skin\", \"sleep\", \"guessed\", \"smart\", \"cleaned\", \"bathroom\", \"touching\", \"flush\", \"plastic\", \"toes\", \"fed\", \"panels\", \"lumber\", \"ham\", \"till\", \"salt\", \"cattle\", \"feed\", \"animal\", \"brush\", \"knees\", \"pipes\", \"pale\", \"drinking\", \"holes\", \"installation\", \"thickness\", \"inexpensive\", \"milk\", \"muscles\", \"width\", \"lengths\", \"grip\", \"windows\", \"tapered\", \"chart\", \"alternate\", \"hadn\", \"wasn\", \"pale\", \"couldn\", \"looked\", \"hair\", \"clothes\", \"cousin\", \"sleep\", \"stared\", \"lips\", \"hell\", \"bottle\", \"sat\", \"wondered\", \"cooling\", \"baby\", \"cup\", \"susan\", \"asleep\", \"sorry\", \"loved\", \"smooth\", \"laughed\", \"brace\", \"didn\", \"yes\", \"fingers\", \"drill\", \"skin\", \"bed\", \"eyes\", \"oh\", \"knew\", \"wouldn\", \"thought\", \"stood\", \"maybe\", \"woman\", \"ll\", \"alexander\", \"felt\", \"voice\", \"love\", \"don\", \"morning\", \"said\", \"door\", \"got\", \"mother\", \"went\", \"little\", \"let\", \"pool\"], \"Freq\": [285.0, 208.0, 192.0, 181.0, 144.0, 139.0, 135.0, 134.0, 127.0, 125.0, 122.0, 121.0, 116.0, 116.0, 117.0, 110.0, 111.0, 117.0, 106.0, 106.0, 105.0, 112.0, 109.0, 102.0, 101.0, 99.0, 99.0, 99.0, 99.0, 98.0, 0.006721489549948892, 0.006721508406472252, 0.006722024141768997, 0.006721655766917393, 0.006721892105407686, 0.006721472855542389, 0.006721654523308678, 0.006721969498917126, 0.006722039523239734, 0.006721619242526917, 0.006721557943211061, 0.006722008131024859, 0.006721449842288827, 0.006721660079608978, 0.006721694503161196, 0.006721542444062494, 0.006721468178731966, 0.006721897119603323, 0.006722194625328183, 0.00672211869399733, 0.006722197172053387, 0.006722234730207019, 0.006721735059098589, 0.006721623676695045, 0.006722011704198407, 0.006722390647206166, 0.006722125682939302, 0.006722029044867488, 0.006721993943282353, 0.006722579534636393, 0.006727166704005456, 0.00672461898419834, 0.006723979987602423, 0.006723831396428295, 0.006723797071937081, 0.00672373234730984, 0.006723721305398042, 0.006723534626483409, 0.006723489979712407, 0.006723446640069653, 0.006723126064538226, 0.006723006659752077, 0.006722866686004352, 0.00672286560850886, 0.006722848033494213, 0.0067228061585779654, 0.006722759647310546, 0.0067227147872477196, 0.006722687003091846, 0.006722680397093061, 0.006722670077578148, 0.0067226676817459625, 0.006722666337784404, 0.006722647665992601, 0.006722643979950295, 0.006722632711943047, 0.006722575289536214, 0.006722567434644879, 0.006722564576272207, 0.006721489549845166, 0.006721508406458884, 0.006722024141665271, 0.006721655767020895, 0.006721892105303962, 0.0067214728554386705, 0.006721654523204958, 0.006721969498813391, 0.006722039523136008, 0.006721619242423192, 0.006721557943107341, 0.006722008130921132, 0.006721449842185111, 0.0067216600795052565, 0.006721694503057477, 0.006721542443958775, 0.006721468178628249, 0.006721897119576373, 0.006722194625224374, 0.00672211869389134, 0.0067221971719496594, 0.00672223473023995, 0.0067217350589948675, 0.006721623676591326, 0.006722011704181802, 0.006722390647102436, 0.006722125682835574, 0.006722029044763762, 0.006721993942953852, 0.006722579534532586, 0.0067271667039016545, 0.0067246189840945774, 0.0067239799874987205, 0.006723831396324387, 0.00672379707183326, 0.006723732347206072, 0.006723721305386797, 0.006723534626381919, 0.006723489979612309, 0.006723446639973338, 0.006723126064429182, 0.006723006659690754, 0.006722866685900613, 0.006722865608405061, 0.006722848033390475, 0.00672280615847418, 0.006722759647206809, 0.006722714787030653, 0.006722687003220517, 0.006722680396989288, 0.006722670077481562, 0.006722667681655119, 0.00672266633768069, 0.006722647665888867, 0.0067226439798464925, 0.006722632711839273, 0.006722575289432481, 0.00672256743479629, 0.006722564576170592, 207.40231671241264, 192.0493284574477, 180.4896974680017, 144.07407348124067, 138.94943533921412, 134.73361458981046, 133.54743180998693, 127.04854137032017, 121.4542577151189, 116.21163074606362, 115.72119391865247, 109.81717480719836, 106.274406073247, 106.09958092941916, 120.36187876839162, 104.90058891988964, 100.71467653992866, 99.134613501954, 98.77303040022527, 98.39766315712656, 98.04358313925376, 111.14230963347224, 98.68490441978999, 95.91497355623385, 101.85145290375749, 95.32606139421283, 92.98677163288667, 92.88592342062688, 97.38922185465378, 88.28549548119834, 116.32289296448697, 279.38655671431303, 111.05676957994845, 115.5953109777118, 123.22988025162088, 107.36459632824236, 99.79791510906848, 0.006721489549797262, 0.006721508406528446, 0.006722024141617345, 0.006721655767242426, 0.006721892105256037, 0.006721472855390749, 0.006721654523157034, 0.006721969498765487, 0.006722039523088081, 0.006721619242375282, 0.006721557943059418, 0.006722008130873206, 0.006721449842137188, 0.006721660079457334, 0.006721694503009552, 0.006721542443910852, 0.006721468178580326, 0.006721897119628279, 0.006722194625176499, 0.006722118693843386, 0.006722197171901731, 0.006722234730369923, 0.006721735058946943, 0.006721623676543402, 0.006722011704247188, 0.006722390647054505, 0.0067221256827876465, 0.006722029044715835, 0.006721993942967156, 0.006722579534484702, 0.006727166703853685, 0.006724618984046629, 0.006723979987450743, 0.006723831396277029, 0.006723797071785238, 0.00672373234715813, 0.006723721305459142, 0.006723534626334085, 0.006723489979573385, 0.006723446639932714, 0.0067231260643823435, 0.006723006659730942, 0.006722866685852682, 0.006722865608357169, 0.006722848033342542, 0.006722806158426283, 0.006722759647158877, 0.006722714787080013, 0.006722687003035478, 0.006722680396941384, 0.006722670077435176, 0.006722667681646212, 0.006722666337632745, 0.006722647665840933, 0.006722643979798576, 0.006722632711791368, 0.00672257528938455, 0.006722567434641527, 0.0067225645761204945, 3.4120587684416726, 4.728602061624373, 3.0320151088204432, 4.435725322972854, 5.536799185359016, 3.383350824428647, 3.1112030326061455, 3.001137384270668, 2.0035754785846573, 2.138676101985031, 1.9319389619566825, 2.288664280169108, 1.8924261872473949, 3.483132508427019, 2.1013508840325215, 3.18069230020458, 3.329732065861155, 2.416368995451957, 2.9047444144748438, 1.807131320506958, 2.5419340782963884, 2.35108254453729, 2.0660167594384324, 2.0123209980962695, 1.9219422354500086, 6.952726868589364, 3.3493689561719617, 2.2312679108294393, 2.8265185280454483, 1.7464806192136382, 3.087705437881408, 5.301129212486323, 3.0558131342422046, 4.904389543989821, 3.381332650579608, 6.041726999532211, 3.42168837078647, 2.9243881118359365, 3.647410539131445, 6.116471391841381, 2.691337583017973, 4.149483810032206, 3.1995186468240577, 3.571101615288526, 4.396222649793763, 3.1987967646704027, 6.231498166949654, 3.336743607699115, 3.8130143761065396, 3.681950618292442, 3.61283915979905, 3.4538793080238976, 3.2048433532113476, 3.1572598622893397], \"Total\": [285.0, 208.0, 192.0, 181.0, 144.0, 139.0, 135.0, 134.0, 127.0, 125.0, 122.0, 121.0, 116.0, 116.0, 117.0, 110.0, 111.0, 117.0, 106.0, 106.0, 105.0, 112.0, 109.0, 102.0, 101.0, 99.0, 99.0, 99.0, 99.0, 98.0, 9.207384122146324, 9.263373045814529, 9.301022494695077, 9.339113664573045, 9.350995535932134, 9.385440599439642, 9.405026851786928, 9.416670085417676, 9.46510308632311, 9.493987262764287, 9.536729191886115, 9.54431125997954, 9.570328316973475, 9.58638974223594, 9.66518774882822, 9.668591205083477, 9.679291609333744, 9.682119887945886, 9.712543535267436, 9.726219958932475, 9.78113398582503, 9.790903280922311, 9.799774426902987, 9.94412476582905, 9.946707109938991, 9.954340940773216, 10.019028129894506, 10.056388431987934, 10.08374411560047, 10.13084710318525, 20.5126521632471, 14.974835513714345, 14.418033983749957, 29.887902240791146, 18.39852098497303, 16.764109060095596, 13.194607212128819, 12.458595713449506, 19.72660876349968, 52.181699561813055, 14.235114261255177, 16.433270493954442, 15.034183102560057, 10.83165296495268, 12.690356246390465, 11.728012618122374, 17.348027914949856, 14.018506613074326, 12.425908743191455, 12.241629666985329, 15.67005583791494, 17.150426459168443, 12.629654577313842, 12.796614333430844, 14.201916572467582, 16.882458912982827, 10.654128325142716, 12.620198287282168, 14.009853300641684, 9.207384122146324, 9.263373045814529, 9.301022494695077, 9.339113664573045, 9.350995535932134, 9.385440599439642, 9.405026851786928, 9.416670085417676, 9.46510308632311, 9.493987262764287, 9.536729191886115, 9.54431125997954, 9.570328316973475, 9.58638974223594, 9.66518774882822, 9.668591205083477, 9.679291609333744, 9.682119887945886, 9.712543535267436, 9.726219958932475, 9.78113398582503, 9.790903280922311, 9.799774426902987, 9.94412476582905, 9.946707109938991, 9.954340940773216, 10.019028129894506, 10.056388431987934, 10.08374411560047, 10.13084710318525, 20.5126521632471, 14.974835513714345, 14.418033983749957, 29.887902240791146, 18.39852098497303, 16.764109060095596, 13.194607212128819, 12.458595713449506, 19.72660876349968, 52.181699561813055, 14.235114261255177, 16.433270493954442, 15.034183102560057, 10.83165296495268, 12.690356246390465, 11.728012618122374, 17.348027914949856, 14.018506613074326, 12.425908743191455, 12.241629666985329, 15.67005583791494, 17.150426459168443, 12.629654577313842, 12.796614333430844, 14.201916572467582, 16.882458912982827, 10.654128325142716, 12.620198287282168, 14.009853300641684, 208.1060359962224, 192.75065067806977, 181.28124890794993, 144.7754282339506, 139.65071414273572, 135.43482722475875, 134.2489163471982, 127.75137268869305, 122.15624469253866, 116.91610389463447, 116.43054105443949, 110.5181265719025, 106.97539647921909, 106.80061043213983, 121.16890481617219, 105.60474950332645, 101.42063160432807, 99.84029794245481, 99.47626619662671, 99.09857278287672, 98.74502375485608, 111.9395639838456, 99.40483260564957, 96.61622923839137, 102.59632050131539, 96.02932387617766, 93.6998861124963, 93.6153465024124, 98.16264930672213, 88.99096947700397, 117.39547209493932, 285.6382191201614, 112.30085807150718, 117.15512654784419, 125.77662576753441, 109.68069890141757, 101.81059453944167, 9.207384122146324, 9.263373045814529, 9.301022494695077, 9.339113664573045, 9.350995535932134, 9.385440599439642, 9.405026851786928, 9.416670085417676, 9.46510308632311, 9.493987262764287, 9.536729191886115, 9.54431125997954, 9.570328316973475, 9.58638974223594, 9.66518774882822, 9.668591205083477, 9.679291609333744, 9.682119887945886, 9.712543535267436, 9.726219958932475, 9.78113398582503, 9.790903280922311, 9.799774426902987, 9.94412476582905, 9.946707109938991, 9.954340940773216, 10.019028129894506, 10.056388431987934, 10.08374411560047, 10.13084710318525, 20.5126521632471, 14.974835513714345, 14.418033983749957, 29.887902240791146, 18.39852098497303, 16.764109060095596, 13.194607212128819, 12.458595713449506, 19.72660876349968, 52.181699561813055, 14.235114261255177, 16.433270493954442, 15.034183102560057, 10.83165296495268, 12.690356246390465, 11.728012618122374, 17.348027914949856, 14.018506613074326, 12.425908743191455, 12.241629666985329, 15.67005583791494, 17.150426459168443, 12.629654577313842, 12.796614333430844, 14.201916572467582, 16.882458912982827, 10.654128325142716, 12.620198287282168, 14.009853300641684, 13.574586798841388, 18.888638160834603, 12.690356246390465, 19.51183792280996, 25.165049699776315, 15.609397143734206, 14.673388898612558, 14.784447497989516, 9.94412476582905, 10.698555049818934, 9.679291609333744, 11.626069314005388, 9.668591205083477, 17.81192551666044, 10.755963666248165, 16.298791204577146, 17.22763970482079, 12.751613891202112, 15.37260434592362, 9.570328316973475, 13.64752900947255, 12.628111278377395, 11.162172945545784, 11.060169640936873, 10.578187491976255, 38.357051973122545, 18.510197750843453, 12.331536526265301, 15.782938246371739, 9.799774426902987, 17.434806912953256, 31.34577849031029, 17.94969826005771, 32.23256849711754, 20.685600314925495, 43.721469231381626, 22.67461619383801, 18.089833155051423, 26.973679728359578, 65.30708874953703, 16.314178272455013, 36.968497248814856, 26.741339308169287, 36.53043771243239, 73.85574378308408, 28.05510921547283, 285.6382191201614, 33.09030544666611, 67.99318535489115, 59.31581194617736, 61.476987171508114, 87.57665600789038, 53.65287795619768, 37.90472333153383], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.5053, -8.5053, -8.5053, -8.5053, -8.5053, -8.5054, -8.5053, -8.5053, -8.5053, -8.5053, -8.5053, -8.5053, -8.5054, -8.5053, -8.5053, -8.5053, -8.5054, -8.5053, -8.5052, -8.5053, -8.5052, -8.5052, -8.5053, -8.5053, -8.5053, -8.5052, -8.5053, -8.5053, -8.5053, -8.5052, -8.5045, -8.5049, -8.505, -8.505, -8.505, -8.505, -8.505, -8.505, -8.5051, -8.5051, -8.5051, -8.5051, -8.5051, -8.5051, -8.5051, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5053, -8.5053, -8.5053, -8.5053, -8.5053, -8.5054, -8.5053, -8.5053, -8.5053, -8.5053, -8.5053, -8.5053, -8.5054, -8.5053, -8.5053, -8.5053, -8.5054, -8.5053, -8.5052, -8.5053, -8.5052, -8.5052, -8.5053, -8.5053, -8.5053, -8.5052, -8.5053, -8.5053, -8.5053, -8.5052, -8.5045, -8.5049, -8.505, -8.505, -8.505, -8.505, -8.505, -8.505, -8.5051, -8.5051, -8.5051, -8.5051, -8.5051, -8.5051, -8.5051, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -6.3645, -6.4414, -6.5035, -6.7288, -6.765, -6.7958, -6.8047, -6.8546, -6.8996, -6.9437, -6.948, -7.0003, -7.0331, -7.0348, -6.9086, -7.0461, -7.0868, -7.1027, -7.1063, -7.1101, -7.1137, -6.9883, -7.1072, -7.1357, -7.0756, -7.1418, -7.1667, -7.1678, -7.1204, -7.2186, -6.9428, -6.0665, -6.9891, -6.949, -6.8851, -7.0229, -7.096, -8.5053, -8.5053, -8.5053, -8.5053, -8.5053, -8.5054, -8.5053, -8.5053, -8.5053, -8.5053, -8.5053, -8.5053, -8.5054, -8.5053, -8.5053, -8.5053, -8.5054, -8.5053, -8.5052, -8.5053, -8.5052, -8.5052, -8.5053, -8.5053, -8.5053, -8.5052, -8.5053, -8.5053, -8.5053, -8.5052, -8.5045, -8.5049, -8.505, -8.505, -8.505, -8.505, -8.505, -8.505, -8.5051, -8.5051, -8.5051, -8.5051, -8.5051, -8.5051, -8.5051, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -8.5052, -7.0901, -6.7638, -7.2082, -6.8277, -6.606, -7.0986, -7.1824, -7.2184, -7.6225, -7.5572, -7.6589, -7.4895, -7.6796, -7.0695, -7.5749, -7.1603, -7.1145, -7.4352, -7.2511, -7.7257, -7.3845, -7.4626, -7.5918, -7.6181, -7.6641, -6.3783, -7.1087, -7.5149, -7.2784, -7.7598, -7.19, -6.6495, -7.2004, -6.7273, -7.0992, -6.5187, -7.0873, -7.2444, -7.0234, -6.5065, -7.3274, -6.8945, -7.1544, -7.0446, -6.8367, -7.1547, -6.4878, -7.1124, -6.979, -7.014, -7.0329, -7.0779, -7.1528, -7.1677], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.008, 1.0019, 0.998, 0.9938, 0.9926, 0.9889, 0.9868, 0.9856, 0.9805, 0.9774, 0.9729, 0.9721, 0.9693, 0.9677, 0.9595, 0.9591, 0.958, 0.9578, 0.9547, 0.9533, 0.9477, 0.9467, 0.9457, 0.9311, 0.9308, 0.9301, 0.9236, 0.9199, 0.9172, 0.9126, 0.2078, 0.5221, 0.5599, -0.1691, 0.3161, 0.4091, 0.6485, 0.7059, 0.2463, -0.7264, 0.5725, 0.4289, 0.5179, 0.8457, 0.6874, 0.7662, 0.3747, 0.5878, 0.7084, 0.7233, 0.4764, 0.3862, 0.6921, 0.679, 0.5748, 0.4019, 0.8622, 0.6929, 0.5884, 1.008, 1.0019, 0.998, 0.9938, 0.9926, 0.9889, 0.9868, 0.9856, 0.9805, 0.9774, 0.9729, 0.9721, 0.9693, 0.9677, 0.9595, 0.9591, 0.958, 0.9578, 0.9547, 0.9533, 0.9477, 0.9467, 0.9457, 0.9311, 0.9308, 0.9301, 0.9236, 0.9199, 0.9172, 0.9126, 0.2078, 0.5221, 0.5599, -0.1691, 0.3161, 0.4091, 0.6485, 0.7059, 0.2463, -0.7264, 0.5725, 0.4289, 0.5179, 0.8457, 0.6874, 0.7662, 0.3747, 0.5878, 0.7084, 0.7233, 0.4764, 0.3862, 0.6921, 0.679, 0.5748, 0.4019, 0.8622, 0.6929, 0.5884, 0.0308, 0.0306, 0.0298, 0.0294, 0.0292, 0.029, 0.029, 0.0287, 0.0285, 0.0282, 0.0281, 0.0279, 0.0276, 0.0276, 0.0275, 0.0275, 0.0272, 0.0271, 0.0271, 0.0271, 0.0271, 0.0271, 0.027, 0.0269, 0.0269, 0.0269, 0.0266, 0.0264, 0.0263, 0.0263, 0.025, 0.0121, 0.0231, 0.0208, 0.0138, 0.0129, 0.0143, 1.008, 1.0019, 0.998, 0.9938, 0.9926, 0.9889, 0.9868, 0.9856, 0.9805, 0.9774, 0.9729, 0.9721, 0.9693, 0.9677, 0.9595, 0.9591, 0.958, 0.9578, 0.9547, 0.9533, 0.9477, 0.9467, 0.9457, 0.9311, 0.9308, 0.9301, 0.9236, 0.9199, 0.9172, 0.9126, 0.2078, 0.5221, 0.5599, -0.1691, 0.3161, 0.4091, 0.6485, 0.7059, 0.2463, -0.7264, 0.5725, 0.4289, 0.5179, 0.8457, 0.6874, 0.7662, 0.3747, 0.5878, 0.7084, 0.7233, 0.4764, 0.3862, 0.6921, 0.679, 0.5748, 0.4019, 0.8622, 0.6929, 0.5884, 2.035, 2.031, 1.9843, 1.9346, 1.9019, 1.8869, 1.8649, 1.8213, 1.8139, 1.806, 1.8045, 1.7906, 1.7849, 1.784, 1.783, 1.7819, 1.7723, 1.7525, 1.7497, 1.749, 1.7353, 1.7349, 1.729, 1.7119, 1.7105, 1.7081, 1.7064, 1.7063, 1.696, 1.6912, 1.6849, 1.6388, 1.6454, 1.5331, 1.6048, 1.4368, 1.5248, 1.5937, 1.4151, 1.0478, 1.6139, 1.2288, 1.2927, 1.0907, 0.5946, 1.2445, -0.4092, 1.1217, 0.5349, 0.6365, 0.5818, 0.1829, 0.5981, 0.9306]}, \"token.table\": {\"Topic\": [3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5], \"Freq\": [0.9967893987560738, 0.007383625175970916, 0.8581492592635027, 0.1838891269850363, 0.9279183529640935, 0.0713783348433918, 0.994118476593406, 0.007827704540105559, 0.9132346787958785, 0.0702488214458368, 0.8359169858166291, 0.20897924645415727, 0.9192206011815687, 0.10213562235350764, 0.8126475965295697, 0.1741387706849078, 0.7955142200507236, 0.1988785550126809, 0.8029913993253718, 0.17206958556972252, 0.9942734630196969, 0.009469271076378065, 0.8274214754052093, 0.20685536885130232, 0.8508073813993807, 0.18906830697764015, 0.9127823950514463, 0.12170431934019284, 0.9925305553556808, 0.01067237156296431, 0.9959274353666929, 0.01005987308451205, 0.9631660579772773, 0.0506929504198567, 0.9508566923304871, 0.07923805769420725, 0.9266367730883158, 0.10295964145425732, 0.9921644336056556, 0.008553141669014273, 0.8982907207482572, 0.0998100800831397, 0.8262653316201855, 0.20656633290504636, 0.817806989436139, 0.20445174735903476, 0.9888643816015507, 0.011237095245472166, 0.7976051620533211, 0.18406272970461257, 0.7687640733456751, 0.20500375289218, 0.8116637433784277, 0.20291593584460693, 0.7842144598574641, 0.15684289197149281, 0.9884163122718185, 0.008904651461908273, 0.9908820484773004, 0.009347943853559438, 0.8081955834802487, 0.18249577691489488, 0.9253337923675558, 0.1028148658186173, 0.9342536743337728, 0.05415963329471147, 0.9066099449687172, 0.0906609944968717, 0.8236742612224631, 0.19007867566672226, 0.8526593827625822, 0.08526593827625822, 0.8452100232864705, 0.10565125291080882, 0.8294577851380276, 0.15951111252654376, 0.9016485891663057, 0.06935758378202352, 0.9773541381032781, 0.01916380662947604, 0.8926519186834926, 0.10820023256769608, 0.810928952665445, 0.16218579053308899, 0.8883758592280305, 0.09870842880311449, 0.9201387091765576, 0.10223763435295084, 0.8495572136894249, 0.10619465171117812, 0.9901401963201972, 0.017071382695175816, 0.9412708003890584, 0.05882942502431615, 0.9905347066337142, 0.00818623724490673, 0.9153694104359361, 0.07041303157199508, 0.904822058247496, 0.10053578424972177, 0.736670673530447, 0.22100120205913412, 0.7687676781814049, 0.19219191954535123, 0.8947686958029408, 0.11930249277372544, 0.8566123389575144, 0.1070765423696893, 0.7741223415172759, 0.17202718700383907, 0.855523881843297, 0.10694048523041212, 0.9934268629514015, 0.010682009279047328, 0.9222950342506552, 0.1152868792813319, 0.9916065066683555, 0.008933391951967167, 0.988113067139329, 0.008518216096028697, 0.8985731719744878, 0.08168847017949889, 0.9273455695969485, 0.07133427458438066, 0.9952122630368402, 0.010052649121584245, 0.9889143430422185, 0.01009096268410427, 0.931211220755722, 0.0665150871968373, 0.8376620684887253, 0.15512260527568986, 0.8137307376089792, 0.18082905280199538, 0.8345164566753855, 0.20862911416884639, 0.9377480392333378, 0.07814566993611148, 0.9319164582526236, 0.055914987495157414, 0.8688678449677982, 0.10860848062097478, 0.8265067654626292, 0.2066266913656573, 0.9591597102363885, 0.034255703937013875, 0.9034241325053439, 0.0918736405937638, 0.7947530499086507, 0.2384259149725952, 0.9033562712764628, 0.10949772985169247, 0.7918840576834792, 0.15837681153669586, 0.8696351197505485, 0.10870438996881857, 0.9779241512436, 0.023851808566917074, 0.8523840639381001, 0.21309601598452502, 0.8291950440577384, 0.16583900881154767, 0.9881558890786527, 0.010187174114212915, 0.8934237468462551, 0.06381598191758965, 0.8911032856080314, 0.10693239427296378, 0.9440990211988314, 0.0674356443713451, 0.9946852286579402, 0.00480524265052145, 0.992932259041306, 0.005516290328007255, 0.932921407994876, 0.05830758799967975, 0.9924550754405035, 0.01012709260653575, 0.8356686437107703, 0.16713372874215407, 0.975559064372602, 0.018234748866777607, 0.787999943094136, 0.2363999829282408, 0.9368338993623136, 0.06691670709730811, 0.9903530958050206, 0.00825294246504184, 0.9232201246066867, 0.09232201246066866, 0.9262576018347668, 0.09750080019313334, 0.923367773822601, 0.07914580918479437, 0.9946439237416894, 0.006907249470428399, 0.9953118407995687, 0.009048289461814261, 0.9936218868895113, 0.010350227988432409, 0.8636163048204824, 0.1079520381025603, 0.9767600458348717, 0.02100559238354563, 0.8026586808017726, 0.16053173616035452, 0.7859902617998854, 0.16842648467140403, 0.9958526031866073, 0.009859926764223835, 0.9892811504379132, 0.010413485794083297, 0.8426385857263837, 0.21065964643159593, 0.8163453209737025, 0.20408633024342562, 0.8044951354080314, 0.20112378385200785, 0.8601204872435125, 0.10751506090543907, 0.9041281641395049, 0.10045868490438943, 0.8388620290074327, 0.20971550725185817, 0.827712839925939, 0.20692820998148476, 0.8062946205820445, 0.17917658235156544, 0.8060067131833947, 0.21982001268638038, 0.8381956311027884, 0.10477445388784855, 0.8412350974585413, 0.18694113276856475, 0.9961055868012425, 0.005188049931256471, 0.9981458595423264, 0.007448849698077062, 0.8379414159681956, 0.1323065393633993, 0.7806094354586093, 0.19515235886465232, 0.8447429696112132, 0.09386032995680148, 0.9925036904854722, 0.009363242363070491, 0.8852471257707606, 0.08047701143370552, 0.8691382212911781, 0.13723235073018603, 0.9094624650114097, 0.1515770775019016, 0.93490175482585, 0.066778696773275, 0.8925256231042377, 0.09916951367824862, 0.9953404166478473, 0.007160722421926959, 0.8974868357722148, 0.11218585447152685, 0.741186308975353, 0.26470939606262606, 0.9822160498361471, 0.01964432099672294, 0.9915835793785478, 0.010015995751298462, 0.9434424598295938, 0.06506499722962716, 0.9501447507167088, 0.0791787292263924, 0.9477292426694902, 0.05923307766684314, 0.8526830685180216, 0.14829270756835158, 0.8367451099004436, 0.18594335775565415, 0.9941877008999778, 0.00974693824411743, 0.9963021639293235, 0.008588811758011409, 0.8506089483923188, 0.10632611854903985, 0.8218277323928479, 0.14502842336344374, 0.8103641139823315, 0.1620728227964663], \"Term\": [\"000\", \"000\", \"alexander\", \"alexander\", \"alternate\", \"alternate\", \"american\", \"american\", \"animal\", \"animal\", \"asleep\", \"asleep\", \"awake\", \"awake\", \"baby\", \"baby\", \"bathroom\", \"bathroom\", \"bed\", \"bed\", \"board\", \"board\", \"bottle\", \"bottle\", \"brace\", \"brace\", \"brush\", \"brush\", \"business\", \"business\", \"car\", \"car\", \"cattle\", \"cattle\", \"chart\", \"chart\", \"chill\", \"chill\", \"city\", \"city\", \"cleaned\", \"cleaned\", \"closet\", \"closet\", \"clothes\", \"clothes\", \"company\", \"company\", \"cooling\", \"cooling\", \"couldn\", \"couldn\", \"cousin\", \"cousin\", \"cup\", \"cup\", \"day\", \"day\", \"development\", \"development\", \"didn\", \"didn\", \"dirty\", \"dirty\", \"don\", \"don\", \"door\", \"door\", \"drill\", \"drill\", \"drinking\", \"drinking\", \"eyed\", \"eyed\", \"eyes\", \"eyes\", \"fed\", \"fed\", \"feed\", \"feed\", \"felt\", \"felt\", \"fingers\", \"fingers\", \"flush\", \"flush\", \"fork\", \"fork\", \"gently\", \"gently\", \"good\", \"good\", \"got\", \"got\", \"government\", \"government\", \"grip\", \"grip\", \"guessed\", \"guessed\", \"hadn\", \"hadn\", \"hair\", \"hair\", \"ham\", \"ham\", \"hated\", \"hated\", \"hell\", \"hell\", \"hey\", \"hey\", \"high\", \"high\", \"holes\", \"holes\", \"home\", \"home\", \"house\", \"house\", \"inexpensive\", \"inexpensive\", \"installation\", \"installation\", \"john\", \"john\", \"kennedy\", \"kennedy\", \"knees\", \"knees\", \"knew\", \"knew\", \"laughed\", \"laughed\", \"laughing\", \"laughing\", \"lengths\", \"lengths\", \"let\", \"let\", \"limp\", \"limp\", \"lips\", \"lips\", \"little\", \"little\", \"ll\", \"ll\", \"looked\", \"looked\", \"love\", \"love\", \"loved\", \"loved\", \"lumber\", \"lumber\", \"man\", \"man\", \"marry\", \"marry\", \"maybe\", \"maybe\", \"men\", \"men\", \"milk\", \"milk\", \"morning\", \"morning\", \"mother\", \"mother\", \"mr\", \"mr\", \"mrs\", \"mrs\", \"muscles\", \"muscles\", \"national\", \"national\", \"oh\", \"oh\", \"old\", \"old\", \"pale\", \"pale\", \"panels\", \"panels\", \"people\", \"people\", \"pipes\", \"pipes\", \"plastic\", \"plastic\", \"pool\", \"pool\", \"president\", \"president\", \"program\", \"program\", \"public\", \"public\", \"rang\", \"rang\", \"said\", \"said\", \"salt\", \"salt\", \"sat\", \"sat\", \"school\", \"school\", \"service\", \"service\", \"shook\", \"shook\", \"skin\", \"skin\", \"sleep\", \"sleep\", \"sloping\", \"sloping\", \"smart\", \"smart\", \"smelled\", \"smelled\", \"smiling\", \"smiling\", \"smooth\", \"smooth\", \"sorry\", \"sorry\", \"sprinkling\", \"sprinkling\", \"stared\", \"stared\", \"state\", \"state\", \"states\", \"states\", \"stood\", \"stood\", \"susan\", \"susan\", \"tapered\", \"tapered\", \"tax\", \"tax\", \"thickness\", \"thickness\", \"thought\", \"thought\", \"till\", \"till\", \"toes\", \"toes\", \"touching\", \"touching\", \"united\", \"united\", \"voice\", \"voice\", \"wasn\", \"wasn\", \"way\", \"way\", \"week\", \"week\", \"went\", \"went\", \"width\", \"width\", \"windows\", \"windows\", \"woman\", \"woman\", \"wondered\", \"wondered\", \"work\", \"work\", \"world\", \"world\", \"worried\", \"worried\", \"wouldn\", \"wouldn\", \"yes\", \"yes\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el226281402759084268806906236936\", ldavis_el226281402759084268806906236936_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el226281402759084268806906236936\", ldavis_el226281402759084268806906236936_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el226281402759084268806906236936\", ldavis_el226281402759084268806906236936_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What conclusions do you draw from the visualization above? Please address the principal component scatterplot and the salient terms graph.\n",
    "\n",
    "A: For each of the topics, the bars displayed on the right indicate how well defined the topic is. For topic 0, the bars decrease in length much quicker than they do in topic 1. This is indicative that topic 0 is much more defined than topic 1. When hovering over the scatterplot on the left, the word frequency in the overall topic is highlighted in red. We can use this in comparison to the above display_topic function to determine the word frequency in both the topics and overall term frequency. The scatterploy does not have any overlap between topics which seems very stange since many words are common english words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
